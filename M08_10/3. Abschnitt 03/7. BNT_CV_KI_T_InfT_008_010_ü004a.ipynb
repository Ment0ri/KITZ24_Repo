{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hcmERY3R2pgs"
   },
   "source": [
    "#  **Dataleakage**\n",
    "\n",
    "Jason Brownlee - Data Preparation for Machine Learning - Data Cleaning, Feature Selection, and Data (2020, machine learning mastery)\n",
    "\n",
    "Chapter 4\n",
    " Data Preparation Without Data Leakage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZiaeflJx3Xks"
   },
   "source": [
    "## train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0ubKCcLt2pA2",
    "outputId": "0f54fce3-ada7-4d06-f8c1-1c76695bead6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 85.152 %\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Bibliotheken importieren\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Step 2: Datensatz definieren\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=7)\n",
    "\n",
    "# Step 3: Split in Trainings- und Testdaten (vor Data Preparation)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
    "\n",
    "# Step 4: Datenvorbereitung (Scaler nur auf die Trainingsdaten anwenden)\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Den Scaler nur auf die Trainingsdaten fitten und anwenden\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "\n",
    "# Die gleichen Transformationen auf die Testdaten anwenden (ohne erneut zu fitten)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Step 5: Modell trainieren\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Step 6: Vorhersagen treffen\n",
    "yhat = model.predict(X_test)\n",
    "\n",
    "# Step 7: Vorhersagen bewerten\n",
    "accuracy = accuracy_score(y_test, yhat)\n",
    "print('Accuracy: %.3f %%' % (accuracy*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cDvO5uxj3IfJ",
    "outputId": "faf27e84-0daf-4234-a56d-3ff81835b14c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 85.152\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5,\n",
    "random_state=7)\n",
    "# split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
    "# define the scaler\n",
    "scaler = MinMaxScaler()\n",
    "# fit on the training dataset\n",
    "scaler.fit(X_train)\n",
    "# scale the training dataset\n",
    "X_train = scaler.transform(X_train)\n",
    "# scale the test dataset\n",
    "X_test = scaler.transform(X_test)\n",
    "# fit the model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "# evaluate the model\n",
    "yhat = model.predict(X_test)\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_test, yhat)\n",
    "print('Accuracy: %.3f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L6teXEMc3c0c"
   },
   "source": [
    "## k-fold Cross-Validatio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3wVjKsiF3i0X",
    "outputId": "7867bff6-0395-4b1b-e798-d7bf5b3667ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 85.400 (3.489)\n"
     ]
    }
   ],
   "source": [
    "# Import von mean und std aus numpy zur Berechnung der mittleren Genauigkeit und Standardabweichung der Ergebnisse\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "\n",
    "# Import der Funktion zur Erzeugung eines synthetischen Klassifikationsdatensatzes\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Import der Funktionen für Cross-Validation und die Wiederholung von K-Fold-Verfahren\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "\n",
    "# Import des MinMaxScalers zur Skalierung der Features auf einen bestimmten Wertebereich (normalerweise zwischen 0 und 1)\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Import der logistischen Regression, die als Modell verwendet wird\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Import der Pipeline zur Kombination mehrerer Schritte der Vorverarbeitung und Modellierung\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Schritt 1: Datensatz definieren\n",
    "# Hier wird ein synthetischer Datensatz mit 1000 Datenpunkten und 20 Features erstellt\n",
    "# - n_informative=15 bedeutet, dass 15 Features tatsächlich Informationen enthalten, die zur Klassifizierung nützlich sind\n",
    "# - n_redundant=5 bedeutet, dass 5 Features eine lineare Kombination der informativen Features sind (sie sind also redundant)\n",
    "# - random_state=7 sorgt für Reproduzierbarkeit\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=7)\n",
    "\n",
    "# Schritt 2: Pipeline definieren\n",
    "# Die Pipeline wird verwendet, um mehrere Verarbeitungsschritte in einem einzigen Modell zu kombinieren\n",
    "steps = list()  # Liste von Schritten initialisieren\n",
    "\n",
    "# Schritt 2.1: MinMaxScaler zur Liste hinzufügen\n",
    "# Der MinMaxScaler wird verwendet, um die Features auf einen Bereich von 0 bis 1 zu skalieren.\n",
    "steps.append(('scaler', MinMaxScaler()))\n",
    "\n",
    "# Schritt 2.2: Logistische Regression zur Liste hinzufügen\n",
    "# Hier wird das Modell definiert, in diesem Fall eine logistische Regression, die für binäre Klassifikation verwendet wird.\n",
    "steps.append(('model', LogisticRegression()))\n",
    "\n",
    "# Schritt 2.3: Die Pipeline mit den definierten Schritten erstellen\n",
    "# - Die Pipeline führt zuerst die Skalierung der Daten durch (MinMaxScaler) und wendet danach die logistische Regression an.\n",
    "pipeline = Pipeline(steps=steps)\n",
    "\n",
    "# Schritt 3: Evaluationsverfahren definieren (Cross-Validation)\n",
    "# RepeatedStratifiedKFold wird verwendet, um eine stratifizierte K-Fold-Cross-Validation mehrfach durchzuführen.\n",
    "# - n_splits=10 bedeutet, dass der Datensatz in 10 Teile (Folds) aufgeteilt wird.\n",
    "# - n_repeats=3 bedeutet, dass der Cross-Validation-Prozess 3-mal wiederholt wird.\n",
    "# - random_state=1 sorgt dafür, dass die Ergebnisse reproduzierbar sind.\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "# Schritt 4: Modell evaluieren (Cross-Validation)\n",
    "# cross_val_score führt die Cross-Validation auf der Pipeline aus und bewertet das Modell anhand des \"accuracy\" Scores.\n",
    "# - Die Pipeline enthält alle Schritte (Skalierung und Modell)\n",
    "# - X und y sind die Daten und Labels.\n",
    "# - scoring='accuracy' bedeutet, dass die Genauigkeit als Bewertungsmaßstab verwendet wird.\n",
    "# - cv=cv gibt das Evaluationsverfahren an (in diesem Fall RepeatedStratifiedKFold)\n",
    "# - n_jobs=-1 bedeutet, dass alle verfügbaren CPU-Kerne genutzt werden, um die Berechnung parallel durchzuführen.\n",
    "scores = cross_val_score(pipeline, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "\n",
    "# Schritt 5: Modellleistung berichten\n",
    "# - mean(scores) berechnet die durchschnittliche Genauigkeit aus allen Cross-Validation-Ergebnissen.\n",
    "# - std(scores) berechnet die Standardabweichung der Genauigkeit, um zu zeigen, wie stabil die Ergebnisse sind.\n",
    "# - Die Ergebnisse werden als Prozentsatz ausgegeben (daher die Multiplikation mit 100).\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(scores) * 100, std(scores) * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "At2x6A8u3rMm",
    "outputId": "d80ad617-4d73-4944-ec99-06dd6ae2e9eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 85.400 (3.489)\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5,\n",
    "random_state=7)\n",
    "# define the pipeline\n",
    "steps = list()\n",
    "steps.append(('scaler', MinMaxScaler()))\n",
    "steps.append(('model', LogisticRegression()))\n",
    "pipeline = Pipeline(steps=steps)\n",
    "# define the evaluation procedure\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# evaluate the model using cross-validation\n",
    "scores = cross_val_score(pipeline, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "# report performance\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(scores)*100, std(scores)*100))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
